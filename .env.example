# ==========================================
# LLM Configuration (LiteLLM)
# ==========================================

# 1. OPTIONAL: Set the default model here (or pass via --model CLI arg)
# Examples: 
#   gemini/gemini-flash-latest
#   ollama/llama3
#   openrouter/meta-llama/llama-3.1-70b-instruct
LLM_MODEL=gemini/gemini-flash-latest

# ==========================================
# Provider Keys (Uncomment the one you use)
# ==========================================

# GEMINI (Google)
# Get key: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_key_here

# OPENROUTER (Universal)
# Get key: https://openrouter.ai/keys
# OPENROUTER_API_KEY=your_openrouter_key_here

# OPENAI (GPT-5, etc.)
# OPENAI_API_KEY=your_openai_key_here

# OLLAMA (Local)
# No API key needed usually, runs on localhost:11434
